{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Udacity - Machine Learning Engineer Nanodegree Program\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capstone Project\n",
    "Matheus Sena Vasconcelos  \n",
    "Fevereiro 28st, 2020\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.I. Project Overview\n",
    "\n",
    "<p style='text-align: justify; text-indent: 40px;'>\n",
    "    RMS Titanic was designed to be the more luxurious and safest ship built in 20th century. On the night of April 20, the Titanic hit an iceberg and sink in the middle on its journey. Unfortunately, due to the low number of rescue boats, more than a half of the passengers have died. The survive number was only 722 of 2224 in total.\n",
    "</p>\n",
    "\n",
    "<p style='text-align: justify; text-indent: 40px;'>\n",
    "    The project proposal is to build a predictor model that recieves, as input, passenger information (like name, age, gender, socio-economic and class), makes the text preprocessor, guesses if this fictitious passenger would survive or not in Titanic tragedy and return it as a HTTP response. As an experiment, supervised and unsupervised machine learning algorithms will be used to build and improve the model. To go further and receive theses passenger information, an endpoint will be develop using Python Frameworks in order to demonstrate another away to create endpoints, instead of those shown during the Nanodegree Program using AWS.\n",
    "</p>\n",
    "\n",
    "<p style='text-align: justify; text-indent: 40px;'>\n",
    "    The main idea of this project is to put into practice all the machine learning and software engineer knowledge learned during the Machine Learning Engineer Nanodegree Program and join it into my developed skills as a Software Developer.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.II. Problem Statement\n",
    "\n",
    "<p style='text-align: justify; text-indent: 40px;'>\n",
    "    The problem is Kaggle challenge and can be access <a href=\"https://www.kaggle.com/c/titanic\">here</a>. Based on the passenger data, the challenge is to build a predictive model that answers the question: \n",
    "</p>\n",
    "\n",
    ">  “what sorts of people were more likely to survive?”\n",
    "\n",
    "<p style='text-align: justify; text-indent: 40px;'>\n",
    "    In others words. The idea is to use the provided dataset, which contains all informations about the passenger aboard Titanic in 1912, and build a machine learning model that predicts if the passenger would survive, based on new data received.\n",
    "</p>\n",
    "\n",
    "<p style='text-align: justify; text-indent: 40px;'>\n",
    "    To build a great predictor model, supervised machine learning classification algorithms will be used, like K-Nearest Neighbors (KNN), Naive Bayes, Random Forest and Support Vector Machines (SVM). Due to the labeled dataset provided, which indicates if the passenger survived or not.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.III. Metrics\n",
    "\n",
    "<p style='text-align: justify; text-indent: 40px;'>\n",
    "    For the model evaluation, the follow metrics are used to measure how good the model is.\n",
    "</p>\n",
    "\n",
    "* __Accuracy Score__: value that indicates how many predicts the model guessed that the passenger would survived and guessed right, comparing to the total data sent. In other words, is the total number of True Positive and True Negatives divided by the total number of samples.\n",
    "\n",
    "<p style='text-align: center;'>\n",
    "    <img src=\"../images/accuracy-formula.png\" alt=\"Accuracy Formula\" width=\"50%\">\n",
    "</p>\n",
    "\n",
    "* __Confusion Matrix__: matrix that indicates how many True Negatives, False Positives, False Negatives and True Positives. Our goal is to increase the number of True Negatives and True Positives, which show that the model more guessing right than wrong.\n",
    "\n",
    "<p style='text-align: center;'>\n",
    "    <img src=\"../images/confusion-matrix-template.jpg\" alt=\"Confusion Matrix Template\" width=\"40%\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.I Data Explation\n",
    "\n",
    "#### II.I.I Dataset\n",
    "\n",
    "<p style='text-align: justify; text-indent: 40px;'>\n",
    "    The dataset brings informations about passenger onboard on RMS Titanic that have survived or not on the night of tragedy. Each row contains unique passengers with different information about them, from name and parents onboard to the amount paid on the ticket.\n",
    "</p>\n",
    "\n",
    "<p style='text-align: center;'>\n",
    "    <img src=\"../images/dataset-head.png\" alt=\"Dataset Head\">\n",
    "</p>\n",
    "\n",
    "<p style='text-align: justify; text-indent: 40px;'>\n",
    "    Over the 890 rows in the dataset, 866 cells have missing values and the most of those values are in the Cabin column. Due to the quantity of null values in that column, it can't be used as features to the model. In other hand, features like Age, which also have null values, but they can be filled out with mean or median. The follow heatmap plot shows that in more details.\n",
    "</p>\n",
    "\n",
    "<p style='text-align: center;'>\n",
    "    <img src=\"../images/plot-missing-values.png\" alt=\"plot missing values\" width=\"50%\">\n",
    "</p>\n",
    "\n",
    "<p style='text-align: justify; text-indent: 40px;'>\n",
    "    Whites lines indicates how many values are missing in each column. It's clear that Cabin column has almost all of its values as empty. While Age column has less than half and Embarked column only one or two data.\n",
    "</p>\n",
    "\n",
    "#### II.I.II Data Statistics\n",
    "\n",
    "<p style='text-align: justify; text-indent: 40px;'>\n",
    "    With Pandas, a Python Library, we can easy extract statistics information in numerical columns in a dataset.\n",
    "</p>\n",
    "\n",
    "<p style='text-align: center;'>\n",
    "    <img src=\"../images/dataset-describe.png\" alt=\"Dataset Describe\" width=\"70%\">\n",
    "</p>\n",
    "\n",
    "<p style='text-align: justify; text-indent: 40px;'>\n",
    "    We can see useful information in the table above. It's noticed that the mean of all ages are equal to 29.69 and the missing values in Age column can be filled with it. Another interesting information is the Survived mean value, only 38.38% of the passenger have survived. It's indicates that we have an imbalanced dataset and have to balance it in data preprocessing step.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.II Exploratory Visualization\n",
    "\n",
    "<p style='text-align: justify; text-indent: 40px;'>\n",
    "    It was said before that the dataset is imbalanced and there are more not survived passenger. The next plot show this counting the number survived and not survived passengers. 0 indicates not survived and 1 indicates survived on the X axis.\n",
    "</p>\n",
    "\n",
    "<p style='text-align: center;'>\n",
    "    <img src=\"../images/plot-counting-survived.png\" alt=\"Couting Survived\" width=\"70%\">\n",
    "</p>\n",
    "\n",
    "<p style='text-align: justify; text-indent: 40px;'>\n",
    "    We can go further and see how many person by sex have survived based on their age.\n",
    "</p>\n",
    "\n",
    "<p style='text-align: center;'>\n",
    "    <img src=\"../images/plot-counting-survived-by-sex.png\" alt=\"Couting Survived By Sex\" width=\"70%\">\n",
    "</p>\n",
    "\n",
    "<p style='text-align: justify; text-indent: 40px;'>\n",
    "    The next plot brings a lot of information about the dataset. The first figure (Age by Pclass) shows the mean, median, min, max and quarters values of the age by passenger class. This kind of plot is useful to use to fill missing values in the Age column. To not drop all null data in Age column, we can fill it with mean age per class, without spoil the entire dataset.\n",
    "</p>\n",
    "<p style='text-align: justify; text-indent: 40px;'>\n",
    "    Other information we can extract of figure is which class had more survived passenger. The Survived by Pclass plot shows that the class with the greter number of death was the Pclass 3. This kind of information indicates that, when the rescue boats arrived after the crashed, they prioritize the more fancy class (Pclass number 1). To reiterate this information, the Fare Paid by Pclass plot show that Pclass 1 had the most expensive fare and, consequently, Pclass 3 the cheapest.\n",
    "</p>\n",
    "\n",
    "<p style='text-align: center;'>\n",
    "    <img src=\"../images/plot-by-pclass.png\" alt=\"by Pclass\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.III Algorithms and Techniques\n",
    "\n",
    "<p style='text-align: justify; text-indent: 40px;'>\n",
    "    <a href=\"https://scikit-learn.org/\">Scikit-Learn</a> is a Python open source machine learning library that provides a bunch of data preprocessing algorithms, statistical estimator and metrics score functions. This library will be used as base of all steps of this report (from preprocessing to predictions).\n",
    "</p>\n",
    "\n",
    "<p style='text-align: justify; text-indent: 40px;'>\n",
    "    For make prediction of Titanic dataset, it will apply four different algorithms: K-Nearest Neighbors (KNN), Naive Bayes, Random Forest and Support Vector Machines (SVM). The model that provides the best result, it will be deployed lately in custom endpoint to make prediction via HTTP request.\n",
    "</p>\n",
    "\n",
    "<p style='text-align: justify; text-indent: 40px;'>\n",
    "    Since our dataset has more than two dimension (more than two features/columns), SVM will form my benchmark. SVM is a great benchmark because it's a common algorithm that provides good results and , in a multidimensional data, it can easily make a regression line. The goal is to reach more than 70% of accuracy and recall in SVM classifier.\n",
    "</p>\n",
    "\n",
    "<p style='text-align: justify; text-indent: 40px;'>\n",
    "    KNN and Naive Bayes algorithm is only to make comparison. The first one makes predictions based on the <a href=\"https://en.wikipedia.org/wiki/Euclidean_distance\">Euclidean distance</a> between points in a Cartesian plane (those points is the dataset features/columns). The second one, also makes predictions based on points in a plane, but instead of use Euclidean distance, it guesses using probability.\n",
    "</p>\n",
    "\n",
    "<p style='text-align: justify; text-indent: 40px;'>\n",
    "    The last algorithm used is Random Forest Classifier, Titanic dataset state-of-the-art classifier. Sklearn library defines this algorithm in a great way:\n",
    "</p>\n",
    "\n",
    "> A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.IV Benchmark\n",
    "\n",
    "<p style='text-align: justify; text-indent: 40px;'>\n",
    "    As mentioned before, SVM it will be our benchmark since it performance well with multidimensional data and can easily reach more than 70% of accuracy. Then, apply other classifier algorithm to get better model and improve its hyperparameters.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.I Data Preprocessing\n",
    "\n",
    "<p style='text-align: justify; text-indent: 40px;'>\n",
    "    First, let's remove columns data we considered useless. Like PassengerId that only contains passenger index and Cabin, because, it was said before, it has a lot of missing data and do not worth fill it with some data.\n",
    "</p>\n",
    "\n",
    "``` python\n",
    ">>> dataset.drop(['Cabin'], axis=1, inplace=True)\n",
    ">>> dataset.drop(['PassengerId', 'Name'], axis=1, inplace=True)\n",
    ">>> dataset.drop(dataset['Embarked'].isna(), axis=0, inplace=True)  # only 2 data are missing\n",
    "```\n",
    "\n",
    "<p style='text-align: justify; text-indent: 40px;'>\n",
    "    On Exploratory Visualization step, we saw that Age was a column that also has missing values. But, differently of Cabin column, we can fill it with data. Considering the Age By Class plot, we are going to fill those missing age data with the mean age value of their passenger class.\n",
    "</p>\n",
    "\n",
    "``` python\n",
    ">>> dataset.loc[dataset[(dataset['Pclass'] == 1) & (dataset['Age'].isna())].index, 'Age'] = 38\n",
    ">>> dataset.loc[dataset[(dataset['Pclass'] == 2) & (dataset['Age'].isna())].index, 'Age'] = 29\n",
    ">>> dataset.loc[dataset[(dataset['Pclass'] == 3) & (dataset['Age'].isna())].index, 'Age'] = 24\n",
    "```\n",
    "\n",
    "<p style='text-align: justify; text-indent: 40px;'>\n",
    "    Also, we can turn the remain string values to number, by mapping them. The Sex column, which has \"male\" and \"female\" values in it, it was turn to 1 and 0, respectively. Embarked column also has string values that can be mapped. \"S\", \"C\" and \"Q\" values was changed to 0, 1 and 2, respectively.\n",
    "</p>\n",
    "\n",
    "``` python\n",
    ">>> gender = {'female': 0, 'male': 1}\n",
    ">>> dataset['Sex'] = dataset['Sex'].map(gender)\n",
    "\n",
    ">>> embarked = {\"S\": 0, \"C\": 1, \"Q\": 2}\n",
    ">>> dataset['Embarked'] = dataset['Embarked'].map(embarked)\n",
    "```\n",
    "\n",
    "<p style='text-align: justify; text-indent: 40px;'>\n",
    "    Ticket column has string type and could also turn into int, but it has 680 unique different kind of tickets in dataset. We can also discard this feature.\n",
    "</p>\n",
    "\n",
    "<p style='text-align: center;'>\n",
    "    <img src=\"../images/dataset-ticket-describe.png\" alt=\"Ticket Describe\" width=\"30%\">\n",
    "</p>\n",
    "\n",
    "``` python\n",
    ">>> dataset.drop(['Ticket'], axis=1, inplace=True)\n",
    "```\n",
    "\n",
    "<p style='text-align: justify; text-indent: 40px;'>\n",
    "    Finally, we can also make more features based on others. We can simplify SibSp and Parch columns in a single column called Relatives, by summing them. Then, we add another column that indicates with this passenger was alone, by verifying the values in the new Relatives column.\n",
    "</p>\n",
    "\n",
    "``` python \n",
    ">>> dataset['Relatives'] = dataset['SibSp'] + dataset['Parch']\n",
    ">>> dataset.loc[dataset['Relatives'] > 0, 'Alone'] = 0\n",
    ">>> dataset.loc[dataset['Relatives'] == 0, 'Alone'] = 1\n",
    ">>> dataset['Alone'] = dataset['Alone'].astype(int)\n",
    ">>> dataset.drop(['SibSp', 'Parch'], axis=1, inplace=True)\n",
    "```\n",
    "\n",
    "<p style='text-align: justify; text-indent: 40px;'>\n",
    "    With all those changes, the dataset leave from:\n",
    "</p>\n",
    "\n",
    "<p style='text-align: center;'>\n",
    "    <img src=\"../images/dataset-head.png\" alt=\"Dataset Head\">\n",
    "</p>\n",
    "\n",
    "<p style='text-align: justify; text-indent: 40px;'>\n",
    "    To:\n",
    "</p>\n",
    "\n",
    "<p style='text-align: center;'>\n",
    "    <img src=\"../images/dataset-head-preproced.png\" alt=\"Dataset Head\">\n",
    "</p>\n",
    "\n",
    "\n",
    "<p style='text-align: justify; text-indent: 40px;'>\n",
    "    <strong>Note:</strong> All the preprocessing steps above was applied in train and test datasets.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.II Train Test Split\n",
    "\n",
    "<p style='text-align: justify; text-indent: 40px;'>\n",
    "    Before put this data in a model, we have to split into a train and test dataset. 85% of the dataset it will be used for training the model, and 25% remaining to evaluation.\n",
    "</p>\n",
    "\n",
    "``` python\n",
    ">>> from sklearn.model_selection import train_test_split\n",
    "\n",
    ">>> X = dataset.drop('Survived', axis=1)\n",
    ">>> Y = dataset.Survived\n",
    ">>> x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=101)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.III Implementation\n",
    "\n",
    "### III.III.I Benchmark Implementation\n",
    "\n",
    "<p style='text-align: justify; text-indent: 40px;'>\n",
    "    As mention before, our benchmark model is a SVM classifier. The predictor will be performed with Sklearn estimators.\n",
    "</p>\n",
    "\n",
    "``` python\n",
    ">>> from sklearn.pipeline import Pipeline\n",
    ">>> from sklearn.preprocessing import MinMaxScaler\n",
    ">>> from sklearn.svm import SVC\n",
    "```\n",
    "\n",
    "<p style='text-align: justify; text-indent: 40px;'>\n",
    "    For the pipeline of the model is used the Sklearn.Pipeline, which joins Sklearn estimator into a unique object. The first step of the pipeline is the Sklearn.MinMaxScaler, which turns each feature of the data into values between 0 and 1 based on the max and the min values of each column. The last step is the own SVM classifier, which receives the normalized data and start the fit process.\n",
    "</p>\n",
    "\n",
    "``` python\n",
    ">>> steps = [('scaller', MinMaxScaler()),\n",
    "             ('classifier', SVC())]\n",
    ">>> benchmark_predictor = Pipeline(steps)\n",
    ">>> benchmark_predictor.fit(x_train, y_train)\n",
    "```\n",
    "\n",
    "<p style='text-align: justify; text-indent: 40px;'>\n",
    "    The accuracy of SVM classifier was <strong>79.82%</strong>, more than expected (70%). But, if we take a look into the confusion matrix, we got a high value of False Positives, which indicates that the model predicts that 37 person survived, but actually they didn't. This number is more than the half of True Positive values.\n",
    "</p>\n",
    "\n",
    "<p style='text-align: center;'>\n",
    "    <img src=\"../images/confusion-matrix-svm.png\" alt=\"Confusion Matrix SVM\">\n",
    "</p>\n",
    "\n",
    "### III.III.II Others Implementation\n",
    "\n",
    "<p style='text-align: justify; text-indent: 40px;'>\n",
    "    To compare the benchmark model with the others one (KNN, Naive Bayes and Random Forest), the same pipeline was used, but the \"classifier\" step was change to the corresponded classifier.\n",
    "</p>\n",
    "\n",
    "<p style='text-align: justify; text-indent: 40px;'>\n",
    "    After all training process, the result is shown in the next table.\n",
    "</p>\n",
    "\n",
    "<p style='text-align: center;'>\n",
    "    <img src=\"../images/dataset-acuracy-all-models.png\" alt=\"Dataset Acuracy All models\">\n",
    "</p>\n",
    "\n",
    "<p style='text-align: justify; text-indent: 40px;'>\n",
    "    It's noticed that the Random Forest Classifier model performance pretty well when comparing to the others. Consequently, it will be our main classifier model.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.IV Refinement\n",
    "\n",
    "<p style='text-align: justify; text-indent: 40px;'>\n",
    "    Comparing the four classifiers, the one with best accuracy was Random Forest, with 82.95% of accuracy. Since it is the best model, let's try to improve its hyperparameters with Grid Search Algorithm.\n",
    "</p>\n",
    "\n",
    "<p style='text-align: justify; text-indent: 40px;'>\n",
    "    Sklearn provides GridSearchCV class. This algorithm consists in create a \"grid\" with all possibles hyperparameters passed (many to many) and get the model with each possibilities. The best model will be the one we are going to use to deploy in Flask Endpoint lately.\n",
    "</p>\n",
    "\n",
    "``` python\n",
    ">>> from sklearn.model_selection import GridSearchCV\n",
    "\n",
    ">>> steps = [('scaller', MinMaxScaler()), \n",
    "             ('classifier', RandomForestClassifier())]\n",
    "\n",
    ">>> param_grid = {\n",
    "    'classifier__n_estimators': [100, 300, 500, 700, 1000],\n",
    "    'classifier__max_depth': [None, 1, 2, 3],\n",
    "    'classifier__criterion': ['gini', 'entropy']\n",
    "    }\n",
    "\n",
    ">>> pipeline = Pipeline(steps)\n",
    ">>> search = GridSearchCV(pipeline, param_grid, n_jobs=-1, scoring='recall')\n",
    ">>> search.fit(x_train, y_train)\n",
    "``` \n",
    "\n",
    "<p style='text-align: justify; text-indent: 40px;'>\n",
    "    After training all possibles models and see the best param, we realize that the best model was the same model with default param we trained previously. Consequently, we got the same accuracy score of the last Random Forest Classifier: <strong>82.95%</strong>.\n",
    "</p>\n",
    "\n",
    "<p style='text-align: justify; text-indent: 40px;'>\n",
    "    Comparing the Random Forest Model with the SVM Benchmark model, we see that we got a better result. The new model decrease the number of False Positives and increase the number of True Positives.\n",
    "</p>\n",
    "    \n",
    "    \n",
    "<p style='text-align: center;'>\n",
    "    <img src=\"../images/confusion-matrix-random-forest.png\" alt=\"Confusion Matrix Random Forest\">\n",
    "</p>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V.I Model Evaluation and Validation\n",
    "\n",
    "### V.II Justification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
